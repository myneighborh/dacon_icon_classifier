{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/10 ---\n",
      "Epoch [1/100] Train Loss: 2.2894 | Val Loss: 2.2205 | Val Acc: 0.1688\n",
      "Epoch [2/100] Train Loss: 2.1909 | Val Loss: 2.3361 | Val Acc: 0.1948\n",
      "Epoch [3/100] Train Loss: 2.0066 | Val Loss: 2.4348 | Val Acc: 0.2078\n",
      "Epoch [4/100] Train Loss: 1.9016 | Val Loss: 1.7161 | Val Acc: 0.4545\n",
      "Epoch [5/100] Train Loss: 1.7673 | Val Loss: 1.4558 | Val Acc: 0.5714\n",
      "Epoch [6/100] Train Loss: 1.6225 | Val Loss: 1.4931 | Val Acc: 0.5584\n",
      "Epoch [7/100] Train Loss: 1.5322 | Val Loss: 1.3302 | Val Acc: 0.5974\n",
      "Epoch [8/100] Train Loss: 1.4601 | Val Loss: 1.3991 | Val Acc: 0.6623\n",
      "Epoch [9/100] Train Loss: 1.3320 | Val Loss: 1.2420 | Val Acc: 0.7013\n",
      "Epoch [10/100] Train Loss: 1.3332 | Val Loss: 1.1224 | Val Acc: 0.7922\n",
      "Epoch [11/100] Train Loss: 1.2437 | Val Loss: 1.2056 | Val Acc: 0.7013\n",
      "Epoch [12/100] Train Loss: 1.1774 | Val Loss: 1.2340 | Val Acc: 0.7532\n",
      "Epoch [13/100] Train Loss: 1.1420 | Val Loss: 0.9486 | Val Acc: 0.8442\n",
      "Epoch [14/100] Train Loss: 1.1720 | Val Loss: 1.0369 | Val Acc: 0.8312\n",
      "Epoch [15/100] Train Loss: 1.1153 | Val Loss: 1.0621 | Val Acc: 0.8052\n",
      "Epoch [16/100] Train Loss: 1.0608 | Val Loss: 0.9662 | Val Acc: 0.8571\n",
      "Epoch [17/100] Train Loss: 1.1187 | Val Loss: 0.9373 | Val Acc: 0.9091\n",
      "Epoch [18/100] Train Loss: 1.0125 | Val Loss: 0.8508 | Val Acc: 0.9351\n",
      "Epoch [19/100] Train Loss: 0.9707 | Val Loss: 0.8619 | Val Acc: 0.9221\n",
      "Epoch [20/100] Train Loss: 0.9393 | Val Loss: 0.9464 | Val Acc: 0.8312\n",
      "Epoch [21/100] Train Loss: 0.9466 | Val Loss: 0.8568 | Val Acc: 0.9481\n",
      "Epoch [22/100] Train Loss: 0.9353 | Val Loss: 0.8025 | Val Acc: 0.9351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 163\u001b[0m\n\u001b[1;32m    161\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    162\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 163\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    164\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    166\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import (\n",
    "    Compose, ToPILImage, Resize, ToTensor, Normalize,\n",
    "    RandomHorizontalFlip, RandomAffine, RandomErasing\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Hyperparameters\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3\n",
    "N_FOLDS = 10\n",
    "SEED = 42\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train['label'] = encoder.fit_transform(train['label'])\n",
    "\n",
    "mean, std = 0.5, 0.5\n",
    "\n",
    "# Augmentation\n",
    "train_transform = Compose([\n",
    "    ToPILImage(),\n",
    "    Resize((64, 64)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[mean], std=[std]),\n",
    "    RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "valid_transform = Compose([\n",
    "    ToPILImage(),\n",
    "    Resize((64, 64)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pixel_df, label_df=None, transform=None):\n",
    "        self.pixel_df = pixel_df.reset_index(drop=True)\n",
    "        self.label_df = label_df.reset_index(drop=True) if label_df is not None else None\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixel_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixel_df.iloc[idx].values.astype(np.uint8).reshape(32, 32)\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.label_df is not None:\n",
    "            label = torch.tensor(self.label_df.iloc[idx], dtype=torch.long)\n",
    "            return image, label\n",
    "        return image\n",
    "\n",
    "# Label Smoothing Loss\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        log_probs = nn.functional.log_softmax(pred, dim=-1)\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(self.smoothing / (pred.size(1) - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.4),  # 여기는 MaxPool 없이 유지\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 8 * 8, 512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# K-Fold\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "all_val_acc = []\n",
    "all_predictions = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train.iloc[:, 2:], train['label'])):\n",
    "    print(f'\\n--- Fold {fold+1}/{N_FOLDS} ---')\n",
    "\n",
    "    train_dataset = CustomDataset(train.iloc[train_idx, 2:], train.iloc[train_idx, 1], transform=train_transform)\n",
    "    valid_dataset = CustomDataset(train.iloc[valid_idx, 2:], train.iloc[valid_idx, 1], transform=valid_transform)\n",
    "    test_dataset = CustomDataset(test.iloc[:, 1:], transform=valid_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = SimpleCNN().to(device)\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=N_EPOCHS, eta_min=1e-5)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        correct, total, val_loss = 0, 0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "        val_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{N_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    all_val_acc.append(val_acc)\n",
    "\n",
    "    # Inference\n",
    "    best_model.eval()\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = best_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_preds.extend(predicted.cpu().numpy())\n",
    "    all_predictions.append(fold_preds)\n",
    "\n",
    "# Majority Voting\n",
    "final_preds = np.array(all_predictions).T\n",
    "ensemble_preds = [np.bincount(row).argmax() for row in final_preds]\n",
    "\n",
    "# Save Submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test['ID'],\n",
    "    'label': encoder.inverse_transform(ensemble_preds)\n",
    "})\n",
    "submission.to_csv('kfold_simplecnn_submission.csv', index=False)\n",
    "print(\"Saved submission as 'kfold_simplecnn_submission.csv'\")\n",
    "\n",
    "\n",
    "# Final Accuracy\n",
    "print(f\"\\nAverage Validation Accuracy over {N_FOLDS} folds: {np.mean(all_val_acc) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
